import os
import re
import tempfile
from collections import Counter
import difflib

import fitz  # PyMuPDF
import streamlit as st

def calculate_text_quality_score(text):
    """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜"""
    
    quality_score = 100
    issues = []
    
    # 1. íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨ ì²´í¬
    special_chars = len(re.findall(r'[^\w\sê°€-í£]', text))
    total_chars = len(text)
    if total_chars > 0:
        special_ratio = special_chars / total_chars
        if special_ratio > 0.1:  # 10% ì´ìƒì´ë©´ ì ìˆ˜ ì°¨ê°
            quality_score -= min(20, special_ratio * 100)
            issues.append(f"íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ ë†’ìŒ ({special_ratio:.1%})")
    
    # 2. ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ íŒ¨í„´ ì²´í¬
    meaningless_patterns = [
        r'[a-zA-Z]{1,2}\s+[a-zA-Z]{1,2}',  # ë‹¨ì¼ ë¬¸ìë“¤ì˜ ì—°ì†
        r'\d+\s+\d+\s+\d+',  # ìˆ«ìë“¤ì˜ ì—°ì†
        r'[^\w\sê°€-í£]{3,}',  # íŠ¹ìˆ˜ë¬¸ì ì—°ì†
    ]
    
    for pattern in meaningless_patterns:
        matches = len(re.findall(pattern, text))
        if matches > 5:
            quality_score -= min(10, matches)
            issues.append(f"ì˜ë¯¸ì—†ëŠ” íŒ¨í„´ ê°ì§€ ({matches}ê°œ)")
    
    # 3. í•œê¸€/ì˜ì–´ ë¹„ìœ¨ ì²´í¬ (Java êµì¬ ê¸°ì¤€)
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    
    if korean_chars + english_chars > 0:
        korean_ratio = korean_chars / (korean_chars + english_chars)
        if korean_ratio < 0.3:  # í•œê¸€ ë¹„ìœ¨ì´ 30% ë¯¸ë§Œì´ë©´
            quality_score -= 15
            issues.append(f"í•œê¸€ ë¹„ìœ¨ ë‚®ìŒ ({korean_ratio:.1%})")
    
    # 4. ì¤„ë°”ê¿ˆ ë¹„ìœ¨ ì²´í¬
    lines = text.split('\n')
    short_lines = [line for line in lines if len(line.strip()) < 10]
    if len(lines) > 0:
        short_line_ratio = len(short_lines) / len(lines)
        if short_line_ratio > 0.5:  # 50% ì´ìƒì´ ì§§ì€ ì¤„ì´ë©´
            quality_score -= 10
            issues.append(f"ì§§ì€ ì¤„ ë¹„ìœ¨ ë†’ìŒ ({short_line_ratio:.1%})")
    
    # 5. ì½”ë“œ ë¸”ë¡ ì™œê³¡ ì²´í¬
    code_indicators = ['class ', 'public ', 'private ', 'System.out', 'import ', 'package ']
    broken_code = 0
    for indicator in code_indicators:
        if indicator in text:
            # ì½”ë“œ í‚¤ì›Œë“œ ì£¼ë³€ì— ì´ìƒí•œ ê³µë°±ì´ë‚˜ ë¬¸ìê°€ ìˆëŠ”ì§€ ì²´í¬
            broken_patterns = [
                indicator.replace(' ', r'\s+'),  # ê³¼ë„í•œ ê³µë°±
                re.escape(indicator) + r'[^\w\sê°€-í£]',  # ì´ìƒí•œ ë¬¸ì
            ]
            for pattern in broken_patterns:
                if re.search(pattern, text):
                    broken_code += 1
    
    if broken_code > 0:
        quality_score -= min(15, broken_code * 3)
        issues.append(f"ì½”ë“œ ë¸”ë¡ ì™œê³¡ ê°ì§€ ({broken_code}ê°œ)")
    
    # ìµœì¢… ì ìˆ˜ ì¡°ì •
    quality_score = max(0, min(100, quality_score))
    
    return {
        'quality_score': round(quality_score),
        'issues': issues,
        'total_chars': total_chars,
        'korean_chars': korean_chars,
        'english_chars': english_chars,
        'line_count': len(lines),
        'short_lines': len(short_lines)
    }

def analyze_extraction_completeness(pages):
    """ì¶”ì¶œ ì™„ì„±ë„ ë¶„ì„ í•¨ìˆ˜"""
    
    total_pages = len(pages)
    empty_pages = sum(1 for page in pages if len(page['content'].strip()) < 50)
    
    # í˜ì´ì§€ë³„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    page_scores = []
    for page in pages:
        quality_info = calculate_text_quality_score(page['content'])
        page_scores.append(quality_info['quality_score'])
    
    avg_quality = sum(page_scores) / len(page_scores) if page_scores else 0
    
    # ì¶”ì¶œ ì™„ì„±ë„ ê³„ì‚°
    completeness = max(0, 100 - (empty_pages / total_pages * 100))
    
    return {
        'total_pages': total_pages,
        'empty_pages': empty_pages,
        'page_scores': page_scores,
        'avg_quality': round(avg_quality),
        'completeness': round(completeness),
        'extraction_grade': get_extraction_grade(avg_quality, completeness)
    }

def get_extraction_grade(quality, completeness):
    """ì¶”ì¶œ í’ˆì§ˆ ë“±ê¸‰ ë°˜í™˜"""
    
    overall_score = (quality + completeness) / 2
    
    if overall_score >= 90:
        return "A (ìš°ìˆ˜)"
    elif overall_score >= 80:
        return "B (ì–‘í˜¸)"
    elif overall_score >= 70:
        return "C (ë³´í†µ)"
    elif overall_score >= 60:
        return "D (ë¯¸í¡)"
    else:
        return "F (ë¶ˆëŸ‰)"

def clean_java_text(text):
    """Java ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    
    patterns = [
        # 1. ì˜ˆì œ ë²ˆí˜¸ ë³µì›
        (r'â–¼\s*ì˜ˆì œ\s+(\d+)\s*-\s*(\d+)\s*/\s*(\w+)\s*\.\s*j(?:ava)?\s*(?=\s|$|[^\w])', r'â–¼ ì˜ˆì œ \1-\2/\3.java'),
        (r'ì˜ˆì œ\s+(\d+)\s*-\s*(\d+)\s*/\s*(\w+)\s*\.\s*j(?:ava)?\s*(?=\s|$|[^\w])', r'ì˜ˆì œ \1-\2/\3.java'),
        
        # 2. íŒŒì¼ëª… íŒ¨í„´
        (r'(\w+)\s*\.\s*j(?:ava)?\s*(?=\s|$|[^\w])', r'\1.java'),
        
        # 3. í´ë˜ìŠ¤ëª… ë³µì›
        (r'(\w+)Ex\s*\.\s*j(?:ava)?\s*(?=\s|$|[^\w])', r'\1Ex.java'),
        (r'(\w+)Test\s*\.\s*j(?:ava)?\s*(?=\s|$|[^\w])', r'\1Test.java'),
        
        # 4. íŒ¨í‚¤ì§€ëª… ë³µì›
        (r'\bj\s*\.\s*util\b', r'java.util'),
        (r'\bj\s*\.\s*io\b', r'java.io'),
        (r'\bj\s*\.\s*awt\b', r'java.awt'),
        
        # 5. API ê´€ë ¨ ë³µì›
        (r'\bJ\s+API\b', r'Java API'),
        (r'\bJava\s+A\s*P\s*I\b', r'Java API'),
        
        # 6. ìˆ«ìì™€ í•˜ì´í”ˆ ì‚¬ì´ ê³µë°± ì œê±°
        (r'(\d+)\s*-\s+(\d+)', r'\1-\2'),
        
        # 7. System.out ê´€ë ¨ ë³µì›
        (r'System\s*\.\s*o+u*t\s*\.\s*print', r'System.out.print'),
        (r'System\s*\.\s*o+u*t\s*\.\s*println', r'System.out.println'),
        
        # 8. Java í‚¤ì›Œë“œ ë³µì›
        (r'\bf+o*a+t\b', r'float'),
        (r'\bi+n+t\b', r'int'),
        (r'\bd+o*u+b+l+e\b', r'double'),
        (r'\bc+h*a+r\b', r'char'),
        (r'\bb+o*o+l+e*a*n\b', r'boolean'),
        
        # 9. í•œêµ­ì–´ ì™¸ë˜ì–´ í‘œê¸°
        (r'ë¦¬ì ¸ë¸Œë“œ', r'ë¦¬ì €ë¸Œë“œ'),
        (r'í´ë¼ìŠ¤', r'í´ë˜ìŠ¤'),
        (r'ë©”ì†Œë“œ', r'ë©”ì„œë“œ'),
        
        # 10. ì£¼ì„ ê´€ë ¨ ë³µì›
        (r'/\s*/\s*', r'// '),
        (r'/\s*\*', r'/*'),
        (r'\*\s*/', r'*/'),
        
    ]
    
    cleaned = text
    
    for pattern, replacement in patterns:
        try:
            cleaned = re.sub(pattern, replacement, cleaned, flags=re.IGNORECASE)
        except Exception:
            continue
    
    # â­ í‘œì‹œ ì œê±° (ì•ë’¤ë¡œ 2ì¤„ì”©)
    cleaned = remove_lines_around_skip_symbol(cleaned)
    
    # ê³µë°± ì •ë¦¬
    cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
    cleaned = re.sub(r'[ \t]+', ' ', cleaned)
    cleaned = re.sub(r'^\s+', '', cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r'\s+$', '', cleaned, flags=re.MULTILINE)
    
    return cleaned

def remove_lines_around_skip_symbol(text):
    """â­ í‘œì‹œê°€ ìˆëŠ” ì¤„ê³¼ ì•ë’¤ 2ì¤„ì”© ì œê±°í•˜ëŠ” í•¨ìˆ˜"""
    
    lines = text.splitlines()
    lines_to_remove = set()
    
    # â­ í‘œì‹œê°€ ìˆëŠ” ì¤„ ì°¾ê¸°
    for i, line in enumerate(lines):
        if 'â­' in line:
            # í˜„ì¬ ì¤„ê³¼ ì•ë’¤ 2ì¤„ì”© ì œê±° ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ
            for j in range(max(0, i-2), min(len(lines), i+3)):
                lines_to_remove.add(j)
    
    # ì œê±° ëŒ€ìƒì´ ì•„ë‹Œ ì¤„ë“¤ë§Œ ë‚¨ê¸°ê¸°
    filtered_lines = []
    for i, line in enumerate(lines):
        if i not in lines_to_remove:
            filtered_lines.append(line)
    
    return '\n'.join(filtered_lines)

def extract_text_from_pdf(pdf_path):
    """PyMuPDFë¡œ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê·¸ë¦¼/ì½”ë“œ ë¸”ë¡ ìŠ¤í‚µ ê¸°ëŠ¥ ì¶”ê°€)"""
    doc = fitz.open(pdf_path)
    pages = []

    def is_image_or_table_block(text):
        # ê·¸ë¦¼/í‘œ ê´€ë ¨ í‚¤ì›Œë“œë§Œ ê°ì§€ (ì½”ë“œ ì˜ˆì‹œëŠ” ë³´ì¡´)
        image_keywords = ["ê·¸ë¦¼", "Figure", "í‘œ", "Table"]
        if any(keyword in text for keyword in image_keywords):
            return True

        # ë§¤ìš° ëª…í™•í•œ ì´ë¯¸ì§€ ë¸”ë¡ë§Œ ê°ì§€ (ì˜ˆ: ì´ë¯¸ì§€ ìº¡ì…˜)
        lines = text.splitlines()
        
        # ì´ë¯¸ì§€ ìº¡ì…˜ íŒ¨í„´ (ì˜ˆ: "ê·¸ë¦¼ 2.1", "Figure 2-1")
        caption_patterns = [
            r'ê·¸ë¦¼\s+\d+[-.]?\d*',
            r'Figure\s+\d+[-.]?\d*',
            r'í‘œ\s+\d+[-.]?\d*',
            r'Table\s+\d+[-.]?\d*'
        ]
        
        for line in lines:
            for pattern in caption_patterns:
                if re.search(pattern, line.strip()):
                    return True

        return False

    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        
        # í˜ì´ì§€ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        text_blocks = page.get_text("blocks")
        
        # ë””ë²„ê¹…: ì›ë³¸ ë¸”ë¡ ìˆœì„œ ì¶œë ¥
        print(f"\n=== í˜ì´ì§€ {page_num + 1} ì›ë³¸ ë¸”ë¡ ìˆœì„œ ===")
        for i, block in enumerate(text_blocks):
            if block[6] == 0:  # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ
                x0, y0, x1, y1, text, block_no, block_type = block
                print(f"ë¸”ë¡ {i}: ì¢Œí‘œ({x0:.0f}, {y0:.0f}) - '{text.strip()[:50]}...'")
        
        # ì¢Œí‘œ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¸”ë¡ ì •ë ¬
        sorted_blocks = sort_blocks_by_reading_order(text_blocks)
        
        page_content = []
        page_word_count = 0

        for block in sorted_blocks:
            text = block[4] # í…ìŠ¤íŠ¸ ë‚´ìš©
            
            if is_image_or_table_block(text):
                print(f"í˜ì´ì§€ {page_num + 1}ì˜ í…ìŠ¤íŠ¸ ë¸”ë¡: '{text.strip()[:50]}...'ì€ ê·¸ë¦¼/í‘œ ë¸”ë¡ìœ¼ë¡œ ì¶”ì •ë˜ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue # ë‹¤ìŒ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë„˜ì–´ê°

            if text.strip(): # ë¹ˆ í…ìŠ¤íŠ¸ ë¸”ë¡ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                page_content.append(text)
                page_word_count += len(text.split())
        
        if page_content: # í˜ì´ì§€ì— ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë¸”ë¡ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            page_text = "\n".join(page_content)
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
            quality_info = calculate_text_quality_score(page_text)
            
            pages.append({
                'page_number': page_num + 1,
                'content': page_text,
                'word_count': page_word_count,
                'quality_score': quality_info['quality_score'],
                'quality_issues': quality_info['issues'],
                'char_stats': {
                    'total_chars': quality_info['total_chars'],
                    'korean_chars': quality_info['korean_chars'],
                    'english_chars': quality_info['english_chars'],
                    'line_count': quality_info['line_count'],
                    'short_lines': quality_info['short_lines']
                }
            })
    
    doc.close()
    return pages

def analyze_text_block_order(page):
    """PyMuPDF í…ìŠ¤íŠ¸ ë¸”ë¡ ì½ê¸° ìˆœì„œ ë¶„ì„ ë° ì‹œê°í™”"""
    
    text_blocks = page.get_text("blocks")
    
    print("=== PyMuPDF í…ìŠ¤íŠ¸ ë¸”ë¡ ì½ê¸° ìˆœì„œ ë¶„ì„ ===")
    
    for i, block in enumerate(text_blocks):
        if block[6] == 0:  # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ
            x0, y0, x1, y1, text, block_no, block_type = block
            print(f"ë¸”ë¡ {i}: ì¢Œí‘œ({x0:.0f}, {y0:.0f}) - ë‚´ìš©: '{text.strip()[:30]}...'")
    
    return text_blocks

def sort_blocks_by_reading_order(text_blocks):
    """í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì½ê¸° ìˆœì„œ(ìœ„â†’ì•„ë˜, ì™¼ìª½â†’ì˜¤ë¥¸ìª½)ë¡œ ì •ë ¬"""
    
    # í…ìŠ¤íŠ¸ ë¸”ë¡ë§Œ í•„í„°ë§
    text_only_blocks = [block for block in text_blocks if block[6] == 0]
    
    # ì¢Œí‘œ ê¸°ë°˜ ì •ë ¬ (yì¢Œí‘œ ìš°ì„ , ê°™ì€ ì¤„ì´ë©´ xì¢Œí‘œ ìˆœ)
    sorted_blocks = sorted(text_only_blocks, key=lambda block: (block[1], block[0]))
    
    print("=== ì •ë ¬ëœ í…ìŠ¤íŠ¸ ë¸”ë¡ ìˆœì„œ ===")
    for i, block in enumerate(sorted_blocks):
        x0, y0, x1, y1, text, block_no, block_type = block
        print(f"ì •ë ¬ í›„ {i}: ì¢Œí‘œ({x0:.0f}, {y0:.0f}) - ë‚´ìš©: '{text.strip()[:30]}...'")
    
    return sorted_blocks

def merge_nearby_blocks(sorted_blocks, distance_threshold=10):
    """ê°€ê¹Œìš´ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ì„ ë³‘í•©"""
    
    merged_blocks = []
    current_group = []
    
    for block in sorted_blocks:
        x0, y0, x1, y1, text, block_no, block_type = block
        
        if not current_group:
            current_group.append(block)
        else:
            # ì´ì „ ë¸”ë¡ê³¼ yì¢Œí‘œ ì°¨ì´ í™•ì¸
            prev_y = current_group[-1][1]
            
            if abs(y0 - prev_y) <= distance_threshold:
                # ê°™ì€ ì¤„ë¡œ íŒë‹¨í•˜ì—¬ ê·¸ë£¹ì— ì¶”ê°€
                current_group.append(block)
            else:
                # ìƒˆ ì¤„ ì‹œì‘, ì´ì „ ê·¸ë£¹ ë³‘í•©
                merged_text = " ".join([b[4].strip() for b in current_group])
                merged_blocks.append(merged_text)
                current_group = [block]
    
    # ë§ˆì§€ë§‰ ê·¸ë£¹ ì²˜ë¦¬
    if current_group:
        merged_text = " ".join([b[4].strip() for b in current_group])
        merged_blocks.append(merged_text)
    
    return merged_blocks

# ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¤ì´ì–´ê·¸ë¨ íŒ¨í„´ ìë™ ê°ì§€
COMMON_PATTERNS = {
    'variable_operations': ['tmp', '=', 'x', 'y'],
    'class_structure': ['class', 'private', 'public'],
    'memory_layout': ['Stack', 'Heap', 'ë©”ëª¨ë¦¬'],
    'inheritance': ['extends', 'ìƒì†', 'ë¶€ëª¨í´ë˜ìŠ¤']
}

def detect_variable_swap_diagram(text):
    """ë³€ìˆ˜ ê°’ êµí™˜ ë‹¤ì´ì–´ê·¸ë¨ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
    for pattern in COMMON_PATTERNS['variable_operations']:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False

def detect_class_structure(text):
    """í´ë˜ìŠ¤ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
    for pattern in COMMON_PATTERNS['class_structure']:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False

def process_diagrams_hybrid(page_data):
    text = page_data['content']
    
    # ìë™ ê°ì§€ ë° ê¸°ë³¸ í…œí”Œë¦¿ ì ìš©
    if detect_variable_swap_diagram(text):
        text += "\n\n[ë‹¤ì´ì–´ê·¸ë¨: ë³€ìˆ˜ ê°’ êµí™˜ ê³¼ì •]\n"
        text += "- 3ë‹¨ê³„ êµí™˜ ê³¼ì • (tmp=x â†’ x=y â†’ y=tmp)\n"
        text += "- ê° ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ìƒíƒœ ë³€í™” í‘œí˜„\n"
        page_data['diagram_type'] = 'variable_swap'
        
    elif detect_class_structure(text):
        text += "\n\n[ë‹¤ì´ì–´ê·¸ë¨: í´ë˜ìŠ¤ êµ¬ì¡°]\n"
        text += "- í´ë˜ìŠ¤ ë©¤ë²„ë³€ìˆ˜ì™€ ë©”ì„œë“œ êµ¬ì¡° í‘œí˜„\n"
        page_data['diagram_type'] = 'class_structure'
    
    page_data['content'] = text
    return page_data

# ì¤‘ìš”í•œ ë‹¤ì´ì–´ê·¸ë¨ë§Œ ìˆ˜ë™ìœ¼ë¡œ ìƒì„¸ ì„¤ëª… ì¶”ê°€
DETAILED_DIAGRAMS = {
    15: {
        'enhanced_description': '''
        ë³€ìˆ˜ ê°’ êµí™˜ ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ê°œë…:
        1. ì„ì‹œë³€ìˆ˜ í•„ìš”ì„±: í•œ ë³€ìˆ˜ì˜ ê°’ì„ ë®ì–´ì“°ê¸° ì „ì— ë°±ì—…
        2. ìˆœì„œì˜ ì¤‘ìš”ì„±: tmpâ†’xâ†’y ìˆœì„œë¥¼ ë°”ê¾¸ë©´ ì•ˆë¨
        3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: ì¶”ê°€ ê³µê°„ O(1) ì‚¬ìš©
        ''',
        'related_code': '''
        // ê¸°ë³¸ êµí™˜ ë°©ë²•
        int tmp = x;
        x = y;
        y = tmp;
        
        // ì‘ìš©: ë°°ì—´ì—ì„œ ë‘ ìš”ì†Œ êµí™˜
        int tmp = arr[i];
        arr[i] = arr[j];
        arr[j] = tmp;
        '''
    }
}

def enhance_educational_content(page_data):
    """êµìœ¡ì  ê°€ì¹˜ë¥¼ ë†’ì´ëŠ” ì½˜í…ì¸  ê°•í™”"""
    
    # 1. ìë™ ë‹¤ì´ì–´ê·¸ë¨ ê°ì§€
    page_data = process_diagrams_hybrid(page_data)
    
    # 2. ì¤‘ìš” í˜ì´ì§€ëŠ” ìˆ˜ë™ ìƒì„¸ ì„¤ëª… ì¶”ê°€
    page_num = page_data['page_number']
    if page_num in DETAILED_DIAGRAMS:
        detail = DETAILED_DIAGRAMS[page_num]
        page_data['content'] += f"\n\n[ìƒì„¸ ì„¤ëª…]\n{detail['enhanced_description']}"
        page_data['content'] += f"\n\n[ê´€ë ¨ ì½”ë“œ]\n{detail['related_code']}"
    
    # 3. RAG ìµœì í™” íƒœê·¸ ì¶”ê°€
    if page_data.get('diagram_type'):
        page_data['content'] += f"\n\n#ì‹œê°í•™ìŠµ #ë‹¤ì´ì–´ê·¸ë¨ #{page_data['diagram_type']}"
    
    return page_data

# í•µì‹¬ ê°œë… í˜ì´ì§€ë§Œ ìˆ˜ë™ìœ¼ë¡œ ìƒì„¸ ì²˜ë¦¬
PRIORITY_PAGES = [15, 23, 45, 67, 89]  # ì¤‘ìš” ë‹¤ì´ì–´ê·¸ë¨ í˜ì´ì§€

# ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì ì§„ì  ê°œì„ 
def collect_diagram_feedback(page_num, user_rating):
    if user_rating < 3:
        # í•´ë‹¹ í˜ì´ì§€ ìˆ˜ë™ ë³´ì™„ í•„ìš”
        add_to_manual_enhancement_queue(page_num)

def main():
    st.set_page_config(page_title="RAG í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ë„êµ¬", page_icon="ğŸ“Š")
    
    st.title("ğŸ“Š RAG í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ë„êµ¬")
    st.write("**RAG ì‹œìŠ¤í…œì„ ìœ„í•œ** PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  Java ê´€ë ¨ íŒŒì¼ëª…ì„ ì •ë¦¬í•©ë‹ˆë‹¤.")
    
    st.success("âœ… **í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„** - ì›ë³¸ ëŒ€ë¹„ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ì¼ì¹˜ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤!")
    
    st.divider()

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”§ ìë™ ì •ë¦¬ ê·œì¹™")
        
        st.subheader("ğŸ“ íŒŒì¼ëª… ë³µì›")
        st.code("""
ì˜ˆì œ 2- 1/VarEx.j â†’ ì˜ˆì œ 2-1/VarEx.java
OverfowEx.j â†’ OverflowEx.java
TestClass.j â†’ TestClass.java
        """, language="text")
        
        st.subheader("ğŸ“¦ íŒ¨í‚¤ì§€ëª… ë³µì›")
        st.code("""
j.util â†’ java.util
J API â†’ Java API
        """, language="text")
        
        st.subheader("ğŸ”§ ì¼ë°˜ ì˜¤íƒ€ ë³µì›")
        st.code("""
foat â†’ float
System.out.print ë³µì›
        """, language="text")
        
        st.divider()
        st.header("ğŸ“Š RAG í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦")
        
        st.subheader("ğŸ¯ í’ˆì§ˆ í‰ê°€ ê¸°ì¤€")
        st.write("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€:")
        st.code("""
â€¢ íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ (10% ì´í•˜)
â€¢ ì˜ë¯¸ì—†ëŠ” íŒ¨í„´ ê°ì§€
â€¢ í•œê¸€/ì˜ì–´ ë¹„ìœ¨ (í•œê¸€ 30% ì´ìƒ)
â€¢ ì§§ì€ ì¤„ ë¹„ìœ¨ (50% ì´í•˜)
â€¢ ì½”ë“œ ë¸”ë¡ ì™œê³¡ ì—¬ë¶€
        """, language="text")
        
        st.subheader("ğŸ† í’ˆì§ˆ ë“±ê¸‰")
        st.write("í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë¶„ë¥˜:")
        st.code("""
Aë“±ê¸‰: 90ì  ì´ìƒ (ìš°ìˆ˜) ğŸŸ¢
Bë“±ê¸‰: 80-89ì  (ì–‘í˜¸) ğŸ”µ
Cë“±ê¸‰: 70-79ì  (ë³´í†µ) ğŸŸ¡
Dë“±ê¸‰: 60-69ì  (ë¯¸í¡) ğŸŸ 
Fë“±ê¸‰: 60ì  ë¯¸ë§Œ (ë¶ˆëŸ‰) ğŸ”´
        """, language="text")
        
        st.subheader("ğŸ” RAG í™œìš©ë„")
        st.write("í’ˆì§ˆ ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ RAG ì‹œìŠ¤í…œì—ì„œ:")
        st.code("""
â€¢ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼
â€¢ í–¥ìƒëœ ì‘ë‹µ í’ˆì§ˆ
â€¢ ë‚®ì€ ì˜¤ë‹µë¥ 
â€¢ íš¨ìœ¨ì ì¸ ë¬¸ì„œ ë¶„í• 
        """, language="text")

    # ì„¤ì • ì˜µì…˜
    col1, col2 = st.columns(2)
    
    with col1:
        enable_text_cleaning = st.checkbox(
            "ğŸ”§ Java í…ìŠ¤íŠ¸ ìë™ ì •ë¦¬", 
            value=True,
            help="Java ê´€ë ¨ í…ìŠ¤íŠ¸ ìë™ ì •ë¦¬"
        )
    
    with col2:
        remove_ebook_text = st.checkbox(
            "ğŸ—‘ï¸ eBook ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì œê±°",
            value=True,
            help="[ebook - ìƒ˜í”Œ] ê´€ë ¨ í…ìŠ¤íŠ¸ ìë™ ì œê±°"
        )
    
    # RAG í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ì„¤ì •
    st.subheader("ğŸ” RAG í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦")
    enable_quality_analysis = st.checkbox(
        "ğŸ“Š í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ í™œì„±í™”",
        value=True,
        help="ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ì—¬ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤"
    )
    
    show_statistics = st.checkbox(
        "ğŸ“Š ìƒì„¸ í†µê³„ í‘œì‹œ",
        value=True,
        help="í˜ì´ì§€ë³„ í†µê³„ ì •ë³´ í‘œì‹œ"
    )

    # íŒŒì¼ ì—…ë¡œë“œ
    st.subheader("ğŸ“¤ PDF íŒŒì¼ ì—…ë¡œë“œ")
    pdf_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type="pdf",
        help="í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤"
    )

    if pdf_file is not None:
        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        st.info(f"ğŸ“„ íŒŒì¼ëª…: {pdf_file.name} ({pdf_file.size:,} bytes)")
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_file_path = tmp_file.name
        
        try:
            with st.spinner("ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                pages = extract_text_from_pdf(tmp_file_path)
            
            if not pages:
                st.error("âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.warning("**ê°€ëŠ¥í•œ ì›ì¸:**")
                st.markdown("""
                - ìŠ¤ìº”ëœ ì´ë¯¸ì§€ PDF (OCR í•„ìš”)
                - ë³´í˜¸ëœ PDF
                - í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì§€ ì•Šì€ PDF
                """)
                
                # ì§ì ‘ ì…ë ¥ ì˜µì…˜
                st.subheader("ğŸ“ ì§ì ‘ í…ìŠ¤íŠ¸ ì…ë ¥")
                direct_text = st.text_area(
                    "í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”:",
                    height=200,
                    placeholder="ì˜ˆ: ì˜ˆì œ 2- 1/VarEx.j\nclass VarEx {\n    // Java ì½”ë“œ\n}"
                )
                
                if direct_text and st.button("ğŸ”§ í…ìŠ¤íŠ¸ ì •ë¦¬í•˜ê¸°"):
                    quality_info = calculate_text_quality_score(direct_text)
                    pages = [{
                        'page_number': 1,
                        'content': direct_text,
                        'word_count': len(direct_text.split()),
                        'quality_score': quality_info['quality_score'],
                        'quality_issues': quality_info['issues'],
                        'char_stats': {
                            'total_chars': quality_info['total_chars'],
                            'korean_chars': quality_info['korean_chars'],
                            'english_chars': quality_info['english_chars'],
                            'line_count': quality_info['line_count'],
                            'short_lines': quality_info['short_lines']
                        }
                    }]
                else:
                    return

            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            if enable_quality_analysis:
                st.subheader("ğŸ“Š RAG í…ìŠ¤íŠ¸ ì¶”ì¶œ í’ˆì§ˆ ë¶„ì„")
                
                # ì „ì²´ í’ˆì§ˆ ë¶„ì„
                extraction_analysis = analyze_extraction_completeness(pages)
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ğŸ“„ ì´ í˜ì´ì§€", extraction_analysis['total_pages'])
                
                with col2:
                    st.metric("ğŸ“Š í‰ê·  í’ˆì§ˆ ì ìˆ˜", f"{extraction_analysis['avg_quality']}/100")
                
                with col3:
                    st.metric("âœ… ì¶”ì¶œ ì™„ì„±ë„", f"{extraction_analysis['completeness']}%")
                
                with col4:
                    grade_color = {
                        "A (ìš°ìˆ˜)": "ğŸŸ¢",
                        "B (ì–‘í˜¸)": "ğŸ”µ", 
                        "C (ë³´í†µ)": "ğŸŸ¡",
                        "D (ë¯¸í¡)": "ğŸŸ ",
                        "F (ë¶ˆëŸ‰)": "ğŸ”´"
                    }
                    grade = extraction_analysis['extraction_grade']
                    st.metric("ğŸ¯ ì¶”ì¶œ ë“±ê¸‰", f"{grade_color.get(grade, 'âšª')} {grade}")
                
                # í’ˆì§ˆ ì´ìŠˆ ìš”ì•½
                all_issues = []
                for page in pages:
                    all_issues.extend(page['quality_issues'])
                
                if all_issues:
                    st.warning("âš ï¸ **ê°ì§€ëœ í’ˆì§ˆ ì´ìŠˆ:**")
                    issue_counter = Counter(all_issues)
                    for issue, count in issue_counter.most_common(5):
                        st.write(f"- {issue}: {count}ê°œ í˜ì´ì§€")
                else:
                    st.success("âœ… í’ˆì§ˆ ì´ìŠˆê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            total_changes = 0
            ebook_removals = 0
            
            if remove_ebook_text or enable_text_cleaning:
                with st.spinner("ğŸ”§ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘..."):
                    for page in pages:
                        original_content = page['content']
                        
                        # eBook ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì œê±°
                        if remove_ebook_text:
                            patterns_to_remove = [
                                # 1. ì •í™•í•œ íŒ¨í„´
                                r"\[ebook\s*-\s*ìƒ˜í”Œ\.\s*ë¬´ë£Œ\s*ê³µìœ \]\s*ì\s*ë°”\s*ì˜\s*ì •ì„\s*4\s*íŒ\s*Java\s*21\s*ì˜¬ì»¬ëŸ¬.*?seong\.namkung@gmail\.com",
                                
                                # 2. ë” ìœ ì—°í•œ íŒ¨í„´
                                r"\[ebook.*?ìƒ˜í”Œ.*?ë¬´ë£Œ.*?ê³µìœ \].*?ìë°”.*?ì •ì„.*?4.*?íŒ.*?Java.*?21.*?seong\.namkung@gmail\.com",
                                
                                # 3. ì´ë©”ì¼ ì£¼ì†Œë§Œ
                                r"seong\.namkung@gmail\.com",
                                
                                # 4. ebook ë¶€ë¶„ë§Œ
                                r"\[ebook.*?ìƒ˜í”Œ.*?ë¬´ë£Œ.*?ê³µìœ \].*?ìë°”.*?ì •ì„.*?",
                                
                                # 5. ì¶œì‹œ ì •ë³´
                                r"2025\.\s*7\.\s*7\s*ì¶œì‹œ",
                                
                                # 6. ì˜¬ì»¬ëŸ¬ ì •ë³´
                                r"ì˜¬ì»¬ëŸ¬.*?2025",
                            ]
                            
                            content_before = page['content']
                            for pattern in patterns_to_remove:
                                page['content'] = re.sub(pattern, "", page['content'], flags=re.IGNORECASE | re.DOTALL)
                            
                            if content_before != page['content']:
                                ebook_removals += 1
                        
                        # Java í…ìŠ¤íŠ¸ ì •ë¦¬
                        if enable_text_cleaning:
                            cleaned_content = clean_java_text(page['content'])
                            if cleaned_content != page['content']:
                                total_changes += 1
                            page['content'] = cleaned_content
                        
                        # ë¹ˆ ì¤„ ì •ë¦¬
                        page['content'] = re.sub(r'\n\s*\n\s*\n+', '\n\n', page['content'])
                        page['content'] = page['content'].strip()
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ì •ë³´ ì—…ë°ì´íŠ¸ (ì •ë¦¬ í›„ ì¬ê³„ì‚°)
            if enable_quality_analysis:
                for page in pages:
                    quality_info = calculate_text_quality_score(page['content'])
                    page['quality_score'] = quality_info['quality_score']
                    page['quality_issues'] = quality_info['issues']
                    page['char_stats'] = {
                        'total_chars': quality_info['total_chars'],
                        'korean_chars': quality_info['korean_chars'],
                        'english_chars': quality_info['english_chars'],
                        'line_count': quality_info['line_count'],
                        'short_lines': quality_info['short_lines']
                    }

            # ê²°ê³¼ í‘œì‹œ
            st.success("âœ… í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ!")
            
            # í†µê³„ ì •ë³´
            if show_statistics:
                st.subheader("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
                col1, col2, col3, col4, col5 = st.columns(5)
                
                total_words = sum(page['word_count'] for page in pages)
                total_text = "\n".join([page['content'] for page in pages])
                java_count = len(re.findall(r'\w+\.java\b', total_text))
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                if enable_quality_analysis:
                    avg_quality = sum(page['quality_score'] for page in pages) / len(pages)
                    low_quality_pages = sum(1 for page in pages if page['quality_score'] < 70)
                else:
                    avg_quality = 0
                    low_quality_pages = 0
                
                col1.metric("ğŸ“„ ì´ í˜ì´ì§€", len(pages))
                col2.metric("ğŸ“ ì´ ë‹¨ì–´", f"{total_words:,}")
                col3.metric("â˜• Java íŒŒì¼", java_count)
                col4.metric("ğŸ“Š í‰ê·  í’ˆì§ˆ", f"{avg_quality:.0f}/100")
                
                if enable_text_cleaning or remove_ebook_text:
                    changes_text = []
                    if enable_text_cleaning and total_changes > 0:
                        changes_text.append(f"Java ì •ë¦¬ {total_changes}ê°œ")
                    if remove_ebook_text and ebook_removals > 0:
                        changes_text.append(f"ìƒ˜í”Œ ì œê±° {ebook_removals}ê°œ")
                    
                    if changes_text:
                        col5.metric("ğŸ”§ ì •ë¦¬ëœ í˜ì´ì§€", " | ".join(changes_text))
                    else:
                        col5.metric("ğŸ”§ ì •ë¦¬ëœ í˜ì´ì§€", "ì—†ìŒ")
                else:
                    col5.metric("âš ï¸ í’ˆì§ˆ ì£¼ì˜", f"{low_quality_pages}ê°œ í˜ì´ì§€")

            # ê²°ê³¼ íƒ­
            tab1, tab2, tab3 = st.tabs(["ğŸ“‹ í˜ì´ì§€ë³„ í’ˆì§ˆ ë¶„ì„", "ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸", "ğŸ’¾ ë‹¤ìš´ë¡œë“œ & í†µê³„"])
            
            with tab1:
                st.subheader("ğŸ“‹ í˜ì´ì§€ë³„ í’ˆì§ˆ ë¶„ì„")
                for page in pages:
                    # í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ì•„ì´ì½˜ ì„¤ì •
                    quality_score = page.get('quality_score', 0)
                    if quality_score >= 90:
                        quality_icon = "ğŸŸ¢"
                    elif quality_score >= 80:
                        quality_icon = "ğŸ”µ"
                    elif quality_score >= 70:
                        quality_icon = "ğŸŸ¡"
                    elif quality_score >= 60:
                        quality_icon = "ğŸŸ "
                    else:
                        quality_icon = "ğŸ”´"
                    
                    with st.expander(f"ğŸ“„ í˜ì´ì§€ {page['page_number']} (ë‹¨ì–´: {page['word_count']:,}ê°œ, í’ˆì§ˆ: {quality_score}/100 {quality_icon})"):
                        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ì •ë³´ í‘œì‹œ
                        if enable_quality_analysis and page.get('quality_issues'):
                            st.warning(f"âš ï¸ **í’ˆì§ˆ ì´ìŠˆ:** {', '.join(page['quality_issues'])}")
                        
                        # ë¬¸ì í†µê³„ ì •ë³´
                        if enable_quality_analysis and page.get('char_stats'):
                            char_stats = page['char_stats']
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                st.caption(f"ğŸ“ ì´ ë¬¸ì: {char_stats['total_chars']:,}")
                                st.caption(f"ğŸ‡°ğŸ‡· í•œê¸€: {char_stats['korean_chars']:,}")
                            
                            with col2:
                                st.caption(f"ğŸ‡ºğŸ‡¸ ì˜ì–´: {char_stats['english_chars']:,}")
                                st.caption(f"ğŸ“ ì´ ì¤„: {char_stats['line_count']:,}")
                            
                            with col3:
                                short_ratio = char_stats['short_lines'] / char_stats['line_count'] if char_stats['line_count'] > 0 else 0
                                st.caption(f"ğŸ“‰ ì§§ì€ ì¤„: {char_stats['short_lines']:,} ({short_ratio:.1%})")
                        
                        # ì´ í˜ì´ì§€ì˜ Java íŒŒì¼ ì°¾ê¸°
                        page_java_files = re.findall(r'\w+\.java\b', page['content'])
                        if page_java_files:
                            st.success(f"â˜• **ì´ í˜ì´ì§€ì˜ Java íŒŒì¼:** {', '.join(set(page_java_files))}")
                        
                        st.text_area(
                            f"í˜ì´ì§€ {page['page_number']} ë‚´ìš©",
                            page['content'],
                            height=300,
                            key=f"page_{page['page_number']}"
                        )
            
            with tab2:
                st.subheader("ğŸ“ ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸")
                full_text = "\n\n".join([
                    f"=== í˜ì´ì§€ {page['page_number']} ===\n{page['content']}"
                    for page in pages
                ])
                st.text_area("ì „ì²´ í…ìŠ¤íŠ¸", full_text, height=600)
            
            with tab3:
                st.subheader("ğŸ’¾ ë‹¤ìš´ë¡œë“œ & RAG í’ˆì§ˆ í†µê³„")
                
                # ë‹¤ìš´ë¡œë“œ
                full_text_for_download = "\n\n".join([
                    f"=== í˜ì´ì§€ {page['page_number']} ===\n{page['content']}"
                    for page in pages
                ])
                
                st.download_button(
                    label="ğŸ“¥ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                    data=full_text_for_download,
                    file_name=f"cleaned_{pdf_file.name.replace('.pdf', '')}.txt",
                    mime="text/plain"
                )
                
                # ìƒì„¸ í†µê³„
                st.subheader("ğŸ“Š ìƒì„¸ í†µê³„")
                total_chars = len(full_text_for_download)
                total_lines = len(full_text_for_download.splitlines())
                
                st.write(f"- **ì´ í˜ì´ì§€:** {len(pages)}")
                st.write(f"- **ì´ ë¬¸ì:** {total_chars:,}")
                st.write(f"- **ì´ ë‹¨ì–´:** {total_words:,}")
                st.write(f"- **ì´ ì¤„:** {total_lines:,}")
                
                # í’ˆì§ˆ í†µê³„
                if enable_quality_analysis:
                    st.subheader("ğŸ“Š RAG í…ìŠ¤íŠ¸ í’ˆì§ˆ í†µê³„")
                    
                    # ì „ì²´ í’ˆì§ˆ ë¶„ì„
                    extraction_analysis = analyze_extraction_completeness(pages)
                    
                    st.write(f"- **í‰ê·  í’ˆì§ˆ ì ìˆ˜:** {extraction_analysis['avg_quality']}/100")
                    st.write(f"- **ì¶”ì¶œ ì™„ì„±ë„:** {extraction_analysis['completeness']}%")
                    st.write(f"- **ì „ì²´ ë“±ê¸‰:** {extraction_analysis['extraction_grade']}")
                    st.write(f"- **ë¹ˆ í˜ì´ì§€:** {extraction_analysis['empty_pages']}ê°œ")
                    
                    # í’ˆì§ˆ ë“±ê¸‰ë³„ í˜ì´ì§€ ë¶„í¬
                    st.subheader("ğŸ“ˆ í’ˆì§ˆ ë“±ê¸‰ë³„ í˜ì´ì§€ ë¶„í¬")
                    grade_distribution = {"A": 0, "B": 0, "C": 0, "D": 0, "F": 0}
                    
                    for page in pages:
                        score = page.get('quality_score', 0)
                        if score >= 90:
                            grade_distribution["A"] += 1
                        elif score >= 80:
                            grade_distribution["B"] += 1
                        elif score >= 70:
                            grade_distribution["C"] += 1
                        elif score >= 60:
                            grade_distribution["D"] += 1
                        else:
                            grade_distribution["F"] += 1
                    
                    for grade, count in grade_distribution.items():
                        if count > 0:
                            percentage = (count / len(pages)) * 100
                            st.write(f"- **{grade}ë“±ê¸‰:** {count}ê°œ í˜ì´ì§€ ({percentage:.1f}%)")
                    
                    # ì£¼ìš” í’ˆì§ˆ ì´ìŠˆ í†µê³„
                    all_issues = []
                    for page in pages:
                        all_issues.extend(page.get('quality_issues', []))
                    
                    if all_issues:
                        st.subheader("âš ï¸ ì£¼ìš” í’ˆì§ˆ ì´ìŠˆ")
                        issue_counter = Counter(all_issues)
                        for issue, count in issue_counter.most_common(10):
                            st.write(f"- **{issue}:** {count}ê°œ í˜ì´ì§€")
                
                # Java íŒŒì¼ ëª©ë¡
                if java_count > 0:
                    st.subheader("â˜• Java íŒŒì¼ ëª©ë¡")
                    java_files = re.findall(r'\w+\.java\b', total_text)
                    unique_java_files = list(set(java_files))
                    
                    for i, java_file in enumerate(unique_java_files[:20], 1):
                        st.write(f"{i}. {java_file}")
                    
                    if len(unique_java_files) > 20:
                        st.write(f"... ì™¸ {len(unique_java_files) - 20}ê°œ")

        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.exception(e)
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(tmp_file_path):
                os.remove(tmp_file_path)

if __name__ == '__main__':
    main()
