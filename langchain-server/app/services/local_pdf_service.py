"""
ë¡œì»¬ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""
import os
import re
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter


def clean_java_text(text: str) -> str:
    """
    OCR ê³¼ì •ì—ì„œ ê¹¨ì§€ê¸° ì‰¬ìš´ Java ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì •ê·œì‹ì„ ì‚¬ìš©í•´ ë³µì›í•©ë‹ˆë‹¤.
    """
    patterns = [
        # 1. ì˜ˆì œ ë²ˆí˜¸ ë³µì› (â–¼ ê¸°í˜¸ ìœ ë¬´ì— ê´€ê³„ì—†ì´ ì²˜ë¦¬)
        (r'â–¼?\s*ì˜ˆì œ\s+(\d+)\s*-\s*(\d+)\s*/\s*(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'â–¼ ì˜ˆì œ \1-\2/\3.java'),
        
        # 2. 'FileName.java'ì™€ ê°™ì€ íŒŒì¼ëª… íŒ¨í„´ ë³µì›
        (r'(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
        
        # 3. 'ClassEx.java', 'ClassTest.java' ê°™ì€ í´ë˜ìŠ¤ëª… ë³µì›
        (r'(\w+(?:Ex|Test))\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
        
        # 4. 'java.util', 'java.io' ê°™ì€ íŒ¨í‚¤ì§€ëª… ë³µì›
        (r'\b(j)\s*\.\s*(util|io|awt)\b', r'java.\2'),
        
        # 5. 'Java API' ê°™ì€ API ê´€ë ¨ ìš©ì–´ ë³µì›
        (r'\bJava\s+A\s*P\s*I\b', r'Java API'),
        
        # 6. System.out.print/println êµ¬ë¬¸ ë³µì›
        (r'System\s*\.\s*o[u\s]*t\s*\.\s*print(ln)?', r'System.out.print\1'),
        
        # 7. Java ê¸°ë³¸ íƒ€ì… í‚¤ì›Œë“œ ë³µì›
        (r'\b[fF]+[oOaA]*[tT]+\b', r'float'),
        (r'\b[iI]+[nN]*[tT]+\b', r'int'),
        (r'\b[dD]+[oO]*[uU]+[bB]+[lL]+[eE]+\b', r'double'),
        (r'\b[cC]+[hH]*[aA]+[rR]+\b', r'char'),
        (r'\b[bB]+[oO]*[lL]+[eEaA]*[nN]+\b', r'boolean'),
    ]
    
    cleaned = text
    for pattern, replacement in patterns:
        try:
            cleaned = re.sub(pattern, replacement, cleaned, flags=re.IGNORECASE)
        except re.error as e:
            print(f"Regex error for pattern '{pattern}': {e}")
            continue
    
    return cleaned


def remove_ebook_sample_text(text: str) -> str:
    """
    PDFì— í¬í•¨ëœ eBook ìƒ˜í”Œ ê´€ë ¨ ìƒìš©êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    patterns = [
        r"[ebook.*?ìƒ˜í”Œ.*?ë¬´ë£Œ.*?ê³µìœ ].*?seong\.namkung@gmail\.com",
        r"seong\.namkung@gmail\.com",
        r"2025\.\s*7\.\s*7\s*ì¶œì‹œ",
        r"ì˜¬ì»¬ëŸ¬.*?2025",
    ]
    
    cleaned_text = text
    for pattern in patterns:
        cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.IGNORECASE | re.DOTALL)
    
    return cleaned_text


class LocalPDFProcessingService:
    """ë¡œì»¬ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        # ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embeddings = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def process_pdf_and_create_chunks(self, pdf_path: str, max_pages: Optional[int] = None) -> List[Document]:
        """
        PDFë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pages_content = self._extract_pdf_text(pdf_path)
        
        if max_pages:
            pages_content = pages_content[:max_pages]
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {max_pages}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
        
        if not pages_content:
            print("âŒ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! {len(pages_content)}ê°œ í˜ì´ì§€")
        
        # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        documents = []
        for page in pages_content:
            doc = Document(
                page_content=page['content'],
                metadata={
                    'page_number': page['page_number'],
                    'word_count': page['word_count'],
                    'source': pdf_path
                }
            )
            documents.append(doc)
        
        # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹ (OpenAI ì˜ì¡´ì„± ì—†ìŒ)
        print("ğŸ”ª í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        return chunks
    
    def _extract_pdf_text(self, pdf_path: str) -> list[dict]:
        """
        PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        pages_content = []
        print(f"ğŸ” PDF íŒŒì¼ ê²½ë¡œ: {pdf_path}")

        with fitz.open(pdf_path) as doc:
            print(f"ğŸ“„ PDF ì´ í˜ì´ì§€ ìˆ˜: {len(doc)}")
            for page_num, page in enumerate(doc, 1):
                text = page.get_text()
                print(f"ğŸ“– í˜ì´ì§€ {page_num}: í…ìŠ¤íŠ¸ ê¸¸ì´ = {len(text.strip())}")
                
                if text and len(text.strip()) > 1:
                    # OCR ì˜¤ë¥˜ ìˆ˜ì • ì ìš©
                    corrected_text = clean_java_text(text)
                    # eBook ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì œê±°
                    final_text = remove_ebook_sample_text(corrected_text)
                    
                    if final_text.strip():
                        pages_content.append({
                            'page_number': page_num,
                            'content': final_text,
                            'word_count': len(final_text.split())
                        })
                        print(f"âœ… í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì™„ë£Œ (ë‹¨ì–´ ìˆ˜: {len(final_text.split())})")
                    else:
                        print(f"âš ï¸ í˜ì´ì§€ {page_num}: ìµœì¢… í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
                else:
                    print(f"âš ï¸ í˜ì´ì§€ {page_num}: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìŒ")
                    
        # ìµœì†Œ 3í˜ì´ì§€ ì´ìƒì˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ ì²˜ë¦¬ ì§„í–‰
        if len(pages_content) < 3:
            print(f"âŒ ìœ íš¨í•œ í˜ì´ì§€ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. (í•„ìš”: 3ê°œ, ì‹¤ì œ: {len(pages_content)}ê°œ)")
            return []
        
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ í˜ì´ì§€: {len(pages_content)}ê°œ")
        return pages_content
    
    def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        return self.embeddings.encode(texts).tolist()
    
    def similarity_search(self, query: str, documents: List[Document], k: int = 5) -> List[Document]:
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        if not documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embeddings.encode([query])[0]
        
        # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
        doc_texts = [doc.page_content for doc in documents]
        doc_embeddings = self.embeddings.encode(doc_texts)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity([query_embedding], doc_embeddings)[0]
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_indices = similarities.argsort()[-k:][::-1]
        
        return [documents[i] for i in sorted_indices]


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_local_pdf_service = None

def get_local_pdf_service():
    global _local_pdf_service
    if _local_pdf_service is None:
        _local_pdf_service = LocalPDFProcessingService()
    return _local_pdf_service 