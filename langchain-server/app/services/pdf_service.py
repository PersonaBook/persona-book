"""
PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""
import os
import re
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_google_genai import GoogleGenerativeAIEmbeddings


def clean_java_text(text: str) -> str:
    """
    OCR ê³¼ì •ì—ì„œ ê¹¨ì§€ê¸° ì‰¬ìš´ Java ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì •ê·œì‹ì„ ì‚¬ìš©í•´ ë³µì›í•©ë‹ˆë‹¤.
    """
    patterns = [
        # 1. ì˜ˆì œ ë²ˆí˜¸ ë³µì› (â–¼ ê¸°í˜¸ ìœ ë¬´ì— ê´€ê³„ì—†ì´ ì²˜ë¦¬)
        (r'â–¼?\s*ì˜ˆì œ\s+(\d+)\s*-\s*(\d+)\s*/\s*(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'â–¼ ì˜ˆì œ \1-\2/\3.java'),
        
        # 2. 'FileName.java'ì™€ ê°™ì€ íŒŒì¼ëª… íŒ¨í„´ ë³µì›
        (r'(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
        
        # 3. 'ClassEx.java', 'ClassTest.java' ê°™ì€ í´ë˜ìŠ¤ëª… ë³µì›
        (r'(\w+(?:Ex|Test))\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
        
        # 4. 'java.util', 'java.io' ê°™ì€ íŒ¨í‚¤ì§€ëª… ë³µì›
        (r'\b(j)\s*\.\s*(util|io|awt)\b', r'java.\2'),
        
        # 5. 'Java API' ê°™ì€ API ê´€ë ¨ ìš©ì–´ ë³µì›
        (r'\bJava\s+A\s*P\s*I\b', r'Java API'),
        
        # 6. System.out.print/println êµ¬ë¬¸ ë³µì›
        (r'System\s*\.\s*o[u\s]*t\s*\.\s*print(ln)?', r'System.out.print\1'),
        
        # 7. Java ê¸°ë³¸ íƒ€ì… í‚¤ì›Œë“œ ë³µì›
        (r'\b[fF]+[oOaA]*[tT]+\b', r'float'),
        (r'\b[iI]+[nN]*[tT]+\b', r'int'),
        (r'\b[dD]+[oO]*[uU]+[bB]+[lL]+[eE]+\b', r'double'),
        (r'\b[cC]+[hH]*[aA]+[rR]+\b', r'char'),
        (r'\b[bB]+[oO]*[lL]+[eEaA]*[nN]+\b', r'boolean'),
    ]
    
    cleaned = text
    for pattern, replacement in patterns:
        try:
            cleaned = re.sub(pattern, replacement, cleaned, flags=re.IGNORECASE)
        except re.error as e:
            print(f"Regex error for pattern '{pattern}': {e}")
            continue
    
    return cleaned


def remove_ebook_sample_text(text: str) -> str:
    """
    PDFì— í¬í•¨ëœ eBook ìƒ˜í”Œ ê´€ë ¨ ìƒìš©êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    patterns = [
        r"[ebook.*?ìƒ˜í”Œ.*?ë¬´ë£Œ.*?ê³µìœ ].*?seong\.namkung@gmail\.com",
        r"seong\.namkung@gmail\.com",
        r"2025\.\s*7\.\s*7\s*ì¶œì‹œ",
        r"ì˜¬ì»¬ëŸ¬.*?2025",
    ]
    
    cleaned_text = text
    for pattern in patterns:
        cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.IGNORECASE | re.DOTALL)
    
    return cleaned_text


class JavaTextbookCleaner:
    """
    Java êµì¬ PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self):
        # ì œê±°í•  ë¼ì¸ íŒ¨í„´
        self.remove_line_patterns = [
            r'^\s*[\|\-\+\s]+$',
            r'^\s*[\d\s\.\,\|\-]+\s*$',
            r'^\s*Chapter\s+\d+\s*$',
            r'^\s*[><]?\s*\d+\s*[><]?$',
        ]
        # í‘œì˜ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” í‚¤ì›Œë“œ íŒ¨í„´
        self.table_content_patterns = [
            r'ì¢…\s*ë¥˜.*?ì—°ì‚°ì.*?ìš°ì„ ìˆœìœ„',
            r'ê²°í•©ê·œì¹™.*?ì—°ì‚°ì',
            r'ìš°ì„ ìˆœìœ„.*?ë†’ìŒ.*?ë‚®ìŒ',
        ]

    def clean_line(self, line: str) -> str:
        """ë¼ì¸ë³„ë¡œ ë¶ˆí•„ìš”í•œ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
        line = line.strip()
        if not line:
            return ""
        
        # ë” ê´€ëŒ€í•œ ì¡°ê±´: 2ê¸€ì ì´ìƒì´ë©´ ìœ ì§€
        if len(line) < 2:
            return ""
        
        for pattern in self.remove_line_patterns:
            if re.match(pattern, line):
                return ""
        
        # íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ë¼ì¸ ì œê±° (ë” ê´€ëŒ€í•˜ê²Œ)
        if re.match(r'^[^\w\sê°€-í£]+$', line) and len(line) < 5:
            return ""
        
        return line

    def is_code_block(self, text: str) -> bool:
        """Java ì½”ë“œ ë¸”ë¡ì¸ì§€ ì¶”ì •í•©ë‹ˆë‹¤."""
        code_indicators = ['class ', 'public ', 'static ', 'void ', 'import ', '//', '/*', '{', '}', ';']
        return any(indicator in text for indicator in code_indicators)

    def is_table_block(self, lines: list[str]) -> bool:
        """ì—¬ëŸ¬ ì¤„ë¡œ êµ¬ì„±ëœ í…ìŠ¤íŠ¸ ë¸”ë¡ì´ í‘œì¸ì§€ ì¶”ì •í•©ë‹ˆë‹¤."""
        if len(lines) < 2:
            return False
        
        full_text = '\n'.join(lines)
        for pattern in self.table_content_patterns:
            if re.search(pattern, full_text, re.DOTALL):
                return True
        
        # ëŒ€ë¶€ë¶„ì˜ ë¼ì¸ì´ ì§§ê³ (70% ì´ìƒ), ìˆ«ì í¬í•¨ ë¼ì¸ì´ ì ˆë°˜ ì´ìƒì´ë©´ í‘œë¡œ ê°„ì£¼
        short_lines = sum(1 for line in lines if len(line.strip()) < 20)
        numeric_lines = sum(1 for line in lines if re.search(r'\d', line))
        if len(lines) > 0 and short_lines / len(lines) > 0.7 and numeric_lines / len(lines) > 0.5:
            return True
        
        return False

    def is_valid_content_block(self, block_text: str) -> bool:
        """ìœ íš¨í•œ ì½˜í…ì¸ (ì½”ë“œ ë˜ëŠ” ì„¤ëª…)ë¥¼ ë‹´ê³  ìˆëŠ” í…ìŠ¤íŠ¸ ë¸”ë¡ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        if not block_text.strip():
            return False
        
        lines = block_text.split('\n')
        if self.is_table_block(lines):
            return False
        
        # ë” ê´€ëŒ€í•œ ì¡°ê±´: í•œê¸€ì´ ìˆê±°ë‚˜, ì½”ë“œì˜ ì¼ë¶€ì´ê±°ë‚˜, ì˜ì–´ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ìœ íš¨í•œ ë¸”ë¡ìœ¼ë¡œ ê°„ì£¼
        if re.search(r'[ê°€-í£]', block_text) or self.is_code_block(block_text):
            return True
        
        # ì˜ì–´ ë¬¸ì¥ì´ ìˆëŠ”ì§€ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
        sentences = re.split(r'[.!?]', block_text)
        return any(len(s.strip()) > 5 for s in sentences if s.strip())


def sort_blocks_by_reading_order(text_blocks: list) -> list:
    """PyMuPDF í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì½ê¸° ìˆœì„œ(ìœ„->ì•„ë˜, ì™¼ìª½->ì˜¤ë¥¸ìª½)ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
    # í…ìŠ¤íŠ¸ ë¸”ë¡(type=0)ë§Œ í•„í„°ë§
    text_only_blocks = [b for b in text_blocks if b[6] == 0]
    # yì¢Œí‘œ(b[1]) ìš°ì„ , ê°™ì€ ë¼ì¸ì´ë©´ xì¢Œí‘œ(b[0]) ìˆœìœ¼ë¡œ ì •ë ¬
    return sorted(text_only_blocks, key=lambda b: (b[1], b[0]))


def extract_preprocessed_pdf_text(pdf_path: str) -> list[dict]:
    """
    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: ì²˜ë¦¬í•  PDF íŒŒì¼ì˜ ê²½ë¡œ

    Returns:
        í˜ì´ì§€ë³„ë¡œ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    cleaner = JavaTextbookCleaner()
    pages_content = []

    with fitz.open(pdf_path) as doc:
        for page_num, page in enumerate(doc, 1):
            text_blocks = page.get_text("blocks")
            sorted_blocks = sort_blocks_by_reading_order(text_blocks)
            
            page_content = []
            for block in sorted_blocks:
                block_text = block[4]
                if cleaner.is_valid_content_block(block_text):
                    cleaned_lines = [cleaner.clean_line(line) for line in block_text.splitlines()]
                    cleaned_block = '\n'.join(filter(None, cleaned_lines))
                    if cleaned_block:
                        # OCR ì˜¤ë¥˜ ìˆ˜ì • ì ìš©
                        corrected_text = clean_java_text(cleaned_block)
                        # eBook ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì œê±°
                        final_text = remove_ebook_sample_text(corrected_text)
                        page_content.append(final_text)
            
            if page_content:
                full_text = "\n\n".join(page_content)
                pages_content.append({
                    'page_number': page_num,
                    'content': full_text,
                    'word_count': len(full_text.split())
                })
    
    return pages_content


class PDFProcessingService:
    """PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    def process_pdf_and_create_chunks(self, pdf_path: str, max_pages: Optional[int] = None) -> List[Document]:
        """
        PDFë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pages_content = extract_preprocessed_pdf_text(pdf_path)
        
        if max_pages:
            pages_content = pages_content[:max_pages]
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {max_pages}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
        
        if not pages_content:
            print("âŒ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! {len(pages_content)}ê°œ í˜ì´ì§€")
        
        # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        documents = []
        for page in pages_content:
            doc = Document(
                page_content=page['content'],
                metadata={
                    'page_number': page['page_number'],
                    'word_count': page['word_count'],
                    'source': pdf_path
                }
            )
            documents.append(doc)
        
        # SemanticChunkerë¡œ ì²­í‚¹
        print("ğŸ”ª ì˜ë¯¸ì  ì²­í‚¹ ì‹œì‘...")
        text_splitter = SemanticChunker(
            self.embeddings,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=70
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        return chunks


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
_pdf_service = None

def get_pdf_service():
    global _pdf_service
    if _pdf_service is None:
        _pdf_service = PDFProcessingService()
    return _pdf_service

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
pdf_service = get_pdf_service 