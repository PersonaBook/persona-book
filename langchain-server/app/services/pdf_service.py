"""
PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""
import os
import re
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings


def clean_java_text(text: str) -> str:
    """
    OCR ê³¼ì •ì—ì„œ ê¹¨ì§€ê¸° ì‰¬ìš´ Java ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì •ê·œì‹ì„ ì‚¬ìš©í•´ ë³µì›í•©ë‹ˆë‹¤.
    """
    patterns = [
        # 1. ì˜ˆì œ ë²ˆí˜¸ ë³µì› (â–¼ ê¸°í˜¸ ìœ ë¬´ì— ê´€ê³„ì—†ì´ ì²˜ë¦¬)
        (r'â–¼?\s*ì˜ˆì œ\s+(\d+)\s*-\s*(\d+)\s*/\s*(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'â–¼ ì˜ˆì œ \1-\2/\3.java'),
        
        # 2. 'FileName.java'ì™€ ê°™ì€ íŒŒì¼ëª… íŒ¨í„´ ë³µì›
        (r'(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
        
        # 3. 'ClassEx.java', 'ClassTest.java' ê°™ì€ í´ë˜ìŠ¤ëª… ë³µì›
        (r'(\w+(?:Ex|Test))\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
        
        # 4. 'java.util', 'java.io' ê°™ì€ íŒ¨í‚¤ì§€ëª… ë³µì›
        (r'\b(j)\s*\.\s*(util|io|awt)\b', r'java.\2'),
        
        # 5. 'Java API' ê°™ì€ API ê´€ë ¨ ìš©ì–´ ë³µì›
        (r'\bJava\s+A\s*P\s*I\b', r'Java API'),
        
        # 6. System.out.print/println êµ¬ë¬¸ ë³µì›
        (r'System\s*\.\s*o[u\s]*t\s*\.\s*print(ln)?', r'System.out.print\1'),
        
        # 7. Java ê¸°ë³¸ íƒ€ì… í‚¤ì›Œë“œ ë³µì›
        (r'\b[fF]+[oOaA]*[tT]+\b', r'float'),
        (r'\b[iI]+[nN]*[tT]+\b', r'int'),
        (r'\b[dD]+[oO]*[uU]+[bB]+[lL]+[eE]+\b', r'double'),
        (r'\b[cC]+[hH]*[aA]+[rR]+\b', r'char'),
        (r'\b[bB]+[oO]*[lL]+[eEaA]*[nN]+\b', r'boolean'),
    ]
    
    cleaned = text
    for pattern, replacement in patterns:
        try:
            cleaned = re.sub(pattern, replacement, cleaned, flags=re.IGNORECASE)
        except re.error as e:
            print(f"Regex error for pattern '{pattern}': {e}")
            continue
    
    return cleaned


def remove_ebook_sample_text(text: str) -> str:
    """
    PDFì— í¬í•¨ëœ eBook ìƒ˜í”Œ ê´€ë ¨ ìƒìš©êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    patterns = [
        r"[ebook.*?ìƒ˜í”Œ.*?ë¬´ë£Œ.*?ê³µìœ ].*?seong\.namkung@gmail\.com",
        r"seong\.namkung@gmail\.com",
        r"2025\.\s*7\.\s*7\s*ì¶œì‹œ",
        r"ì˜¬ì»¬ëŸ¬.*?2025",
    ]
    
    cleaned_text = text
    for pattern in patterns:
        cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.IGNORECASE | re.DOTALL)
    
    return cleaned_text


class JavaTextbookCleaner:
    """
    Java êµì¬ PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self):
        # ì œê±°í•  ë¼ì¸ íŒ¨í„´
        self.remove_line_patterns = [
            r'^\s*[\|\-\+\s]+$',
            r'^\s*[\d\s\.\,\|\-]+\s*$',
            r'^\s*Chapter\s+\d+\s*$',
            r'^\s*[><]?\s*\d+\s*[><]?$',
        ]
        # í‘œì˜ ë‚´ìš©ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” í‚¤ì›Œë“œ íŒ¨í„´
        self.table_content_patterns = [
            r'ì¢…\s*ë¥˜.*?ì—°ì‚°ì.*?ìš°ì„ ìˆœìœ„',
            r'ê²°í•©ê·œì¹™.*?ì—°ì‚°ì',
            r'ìš°ì„ ìˆœìœ„.*?ë†’ìŒ.*?ë‚®ìŒ',
        ]
    
    def clean_line(self, line: str) -> str:
        """ë¼ì¸ì„ ì •ì œí•©ë‹ˆë‹¤."""
        # ì œê±°í•  íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” ë¼ì¸ ì œê±°
        for pattern in self.remove_line_patterns:
            if re.match(pattern, line):
                return ""
        
        # ì•ë’¤ ê³µë°± ì œê±°
        return line.strip()
    
    def is_code_block(self, text: str) -> bool:
        """ì½”ë“œ ë¸”ë¡ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        code_indicators = ['public', 'class', 'void', 'main', 'System.out', 'import', 'package']
        return any(indicator in text for indicator in code_indicators)
    
    def is_table_block(self, lines: list[str]) -> bool:
        """í‘œ ë¸”ë¡ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        text = '\n'.join(lines)
        return any(re.search(pattern, text) for pattern in self.table_content_patterns)
    
    def is_valid_content_block(self, block_text: str) -> bool:
        """ìœ íš¨í•œ ì½˜í…ì¸  ë¸”ë¡ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        if not block_text or len(block_text.strip()) < 10:
            return False
        
        # ì½”ë“œ ë¸”ë¡ì´ë‚˜ í‘œ ë¸”ë¡ì€ ìœ íš¨
        if self.is_code_block(block_text):
            return True
        
        lines = block_text.splitlines()
        if self.is_table_block(lines):
            return True
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ìµœì†Œ ê¸¸ì´ í™•ì¸
        return len(block_text.strip()) >= 20


def sort_blocks_by_reading_order(text_blocks: list) -> list:
    """PyMuPDF í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì½ê¸° ìˆœì„œ(ìœ„->ì•„ë˜, ì™¼ìª½->ì˜¤ë¥¸ìª½)ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
    def get_block_position(block):
        # block[0]: x0, block[1]: y0, block[2]: x1, block[3]: y1
        return (block[1], block[0])  # (y, x) ìˆœì„œë¡œ ì •ë ¬
    
    return sorted(text_blocks, key=get_block_position)


def extract_preprocessed_pdf_text(pdf_path: str) -> list[dict]:
    """
    PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: ì²˜ë¦¬í•  PDF íŒŒì¼ì˜ ê²½ë¡œ

    Returns:
        í˜ì´ì§€ë³„ë¡œ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    cleaner = JavaTextbookCleaner()
    pages_content = []

    with fitz.open(pdf_path) as doc:
        for page_num, page in enumerate(doc, 1):
            text_blocks = page.get_text("blocks")
            sorted_blocks = sort_blocks_by_reading_order(text_blocks)
            
            page_content = []
            for block in sorted_blocks:
                block_text = block[4]
                if cleaner.is_valid_content_block(block_text):
                    cleaned_lines = [cleaner.clean_line(line) for line in block_text.splitlines()]
                    cleaned_block = '\n'.join(filter(None, cleaned_lines))
                    if cleaned_block:
                        # OCR ì˜¤ë¥˜ ìˆ˜ì • ì ìš©
                        corrected_text = clean_java_text(cleaned_block)
                        # eBook ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì œê±°
                        final_text = remove_ebook_sample_text(corrected_text)
                        page_content.append(final_text)
            
            if page_content:
                full_text = "\n\n".join(page_content)
                pages_content.append({
                    'page_number': page_num,
                    'content': full_text,
                    'word_count': len(full_text.split())
                })
    
    return pages_content


class PDFProcessingService:
    """PDF ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
    
    def process_pdf_and_create_chunks(self, pdf_path: str, max_pages: Optional[int] = None) -> List[Document]:
        """
        PDFë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pages_content = extract_preprocessed_pdf_text(pdf_path)
        
        if max_pages:
            pages_content = pages_content[:max_pages]
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {max_pages}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
        
        if not pages_content:
            print("âŒ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! {len(pages_content)}ê°œ í˜ì´ì§€")
        
        # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        documents = []
        for page in pages_content:
            doc = Document(
                page_content=page['content'],
                metadata={
                    'page_number': page['page_number'],
                    'word_count': page['word_count'],
                    'source': pdf_path
                }
            )
            documents.append(doc)
        
        # SemanticChunkerë¡œ ì²­í‚¹
        print("ğŸ”ª ì˜ë¯¸ì  ì²­í‚¹ ì‹œì‘...")
        text_splitter = SemanticChunker(
            self.embeddings,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=70
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        return chunks


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
_pdf_service = None

def get_pdf_service():
    global _pdf_service
    if _pdf_service is None:
        _pdf_service = PDFProcessingService()
    return _pdf_service

# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
pdf_service = get_pdf_service 