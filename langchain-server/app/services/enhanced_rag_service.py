"""
í–¥ìƒëœ RAG ì„œë¹„ìŠ¤ - langchain-server1 ê¸°ë°˜
"""

import os
import re
import json
import fitz  # PyMuPDF
from typing import List, Dict, Any, Optional, Tuple
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from app.core.vector_store import VectorStoreManager
from app.generators.question_generator import QuestionGenerator

class EnhancedRAGService:
    """í–¥ìƒëœ RAG ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            temperature=0.7,
            max_tokens=2000
        )
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="text-embedding-004"
        )
        self.vector_store_manager = VectorStoreManager(self.embeddings)
        self.question_generator = QuestionGenerator(self.llm)
        self.current_chunks = []
        
    def clean_java_text(self, text: str) -> str:
        """OCR ê³¼ì •ì—ì„œ ê¹¨ì§€ê¸° ì‰¬ìš´ Java ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì •ê·œì‹ì„ ì‚¬ìš©í•´ ë³µì›í•©ë‹ˆë‹¤."""
        patterns = [
            # 1. ì˜ˆì œ ë²ˆí˜¸ ë³µì› (â–¼ ê¸°í˜¸ ìœ ë¬´ì— ê´€ê³„ì—†ì´ ì²˜ë¦¬)
            (r'â–¼?\s*ì˜ˆì œ\s+(\d+)\s*-\s*(\d+)\s*/\s*(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'â–¼ ì˜ˆì œ \1-\2/\3.java'),
            
            # 2. 'FileName.java'ì™€ ê°™ì€ íŒŒì¼ëª… íŒ¨í„´ ë³µì›
            (r'(\w+)\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
            
            # 3. 'ClassEx.java', 'ClassTest.java' ê°™ì€ í´ë˜ìŠ¤ëª… ë³µì›
            (r'(\w+(?:Ex|Test))\s*\.\s*j(?:ava)?\s*(?=[^\w]|$)', r'\1.java'),
            
            # 4. 'java.util', 'java.io' ê°™ì€ íŒ¨í‚¤ì§€ëª… ë³µì›
            (r'\b(j)\s*\.\s*(util|io|awt)\b', r'java.\2'),
            
            # 5. 'Java API' ê°™ì€ API ê´€ë ¨ ìš©ì–´ ë³µì›
            (r'\bJava\s+A\s*P\s*I\b', r'Java API'),
            
            # 6. System.out.print/println êµ¬ë¬¸ ë³µì›
            (r'System\s*\.\s*o[u\s]*t\s*\.\s*print(ln)?', r'System.out.print\1'),
            
            # 7. Java ê¸°ë³¸ íƒ€ì… í‚¤ì›Œë“œ ë³µì›
            (r'\b[fF]+[oOaA]*[tT]+\b', r'float'),
            (r'\b[iI]+[nN]*[tT]+\b', r'int'),
            (r'\b[dD]+[oO]*[uU]+[bB]+[lL]+[eE]+\b', r'double'),
            (r'\b[cC]+[hH]*[aA]+[rR]+\b', r'char'),
            (r'\b[bB]+[oO]*[lL]+[eEaA]*[nN]+\b', r'boolean'),
        ]
        
        cleaned = text
        for pattern, replacement in patterns:
            try:
                cleaned = re.sub(pattern, replacement, cleaned, flags=re.IGNORECASE)
            except re.error as e:
                print(f"Regex error for pattern '{pattern}': {e}")
                continue
        
        return cleaned
    
    def remove_ebook_sample_text(self, text: str) -> str:
        """PDFì— í¬í•¨ëœ eBook ìƒ˜í”Œ ê´€ë ¨ ìƒìš©êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        patterns = [
            r"[ebook.*?ìƒ˜í”Œ.*?ë¬´ë£Œ.*?ê³µìœ ].*?seong\.namkung@gmail\.com",
            r"seong\.namkung@gmail\.com",
            r"2025\.\s*7\.\s*7\s*ì¶œì‹œ",
            r"ì˜¬ì»¬ëŸ¬.*?2025",
        ]
        
        cleaned_text = text
        for pattern in patterns:
            cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.IGNORECASE | re.DOTALL)
        
        return cleaned_text
    
    def extract_preprocessed_pdf_text(self, pdf_path: str, max_pages: Optional[int] = None) -> List[Dict[str, Any]]:
        """PDFì—ì„œ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            if max_pages:
                pages_to_process = min(max_pages, total_pages)
            else:
                pages_to_process = total_pages
            
            print(f"ğŸ“„ PDF ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {pages_to_process}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
            
            text_blocks = []
            
            for page_num in range(pages_to_process):
                page = doc[page_num]
                text = page.get_text()
                
                if not text.strip():
                    continue
                
                # í…ìŠ¤íŠ¸ ì •ì œ
                cleaned_text = self.clean_java_text(text)
                cleaned_text = self.remove_ebook_sample_text(cleaned_text)
                
                if len(cleaned_text.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    text_blocks.append({
                        "page": page_num + 1,
                        "content": cleaned_text,
                        "word_count": len(cleaned_text.split())
                    })
                    print(f"âœ… í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì™„ë£Œ (ë‹¨ì–´ ìˆ˜: {len(cleaned_text.split())})")
            
            doc.close()
            return text_blocks
            
        except Exception as e:
            print(f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def create_chunks_from_text_blocks(self, text_blocks: List[Dict[str, Any]]) -> List[Document]:
        """í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        try:
            # SemanticChunker ì‚¬ìš©
            chunker = SemanticChunker(self.embeddings)
            
            all_text = "\n\n".join([block["content"] for block in text_blocks])
            chunks = chunker.split_text(all_text)
            
            # Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "source": "java_textbook",
                        "chunk_id": i,
                        "total_chunks": len(chunks)
                    }
                )
                documents.append(doc)
            
            print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(documents)}ê°œ ì²­í¬")
            return documents
            
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def setup_rag_system(self, pdf_path: str, max_pages: Optional[int] = None) -> Tuple[bool, str]:
        """RAG ì‹œìŠ¤í…œì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
            print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‹œì‘...")
            text_blocks = self.extract_preprocessed_pdf_text(pdf_path, max_pages)
            
            if not text_blocks:
                return False, "PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
            # 2. ì²­í‚¹
            print(f"ğŸ”ª í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘...")
            chunks = self.create_chunks_from_text_blocks(text_blocks)
            
            if not chunks:
                return False, "í…ìŠ¤íŠ¸ ì²­í‚¹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
            # 3. ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • (ì§ì ‘ ì²˜ë¦¬)
            try:
                from langchain_community.vectorstores import ElasticsearchStore
                
                # Elasticsearch ì—°ê²° ì„¤ì • (Docker í™˜ê²½)
                es_url = "http://elasticsearch:9200"
                index_name = "java_learning_docs"
                
                # Elasticsearch ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                self.vector_store_manager.vector_store = ElasticsearchStore.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    es_url=es_url,
                    index_name=index_name
                )
                
                print(f"âœ… Elasticsearch ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì™„ë£Œ: {index_name}")
                
            except Exception as e:
                print(f"âŒ Elasticsearch ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì‹¤íŒ¨: {e}")
                return False, f"ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # 4. ê²€ìƒ‰ê¸° ì„¤ì •
            retriever = self.vector_store_manager.get_retriever(k=5)
            if retriever:
                self.question_generator.retriever = retriever
            
            self.current_chunks = chunks
            return True, f"RAG ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ. {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ë¨."
            
        except Exception as e:
            return False, f"RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def generate_question(self, topic: str, difficulty: str = "ë³´í†µ") -> Optional[Dict[str, Any]]:
        """ì£¼ì œì™€ ë‚œì´ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.question_generator.retriever:
            print("âŒ ê²€ìƒ‰ê¸°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        return self.question_generator.generate_question(topic, difficulty)
    
    def search_relevant_content(self, query: str, k: int = 5) -> List[Document]:
        """ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        return self.vector_store_manager.similarity_search(query, k)
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "total_chunks": len(self.current_chunks),
            "vector_store_ready": self.vector_store_manager.vector_store is not None,
            "retriever_ready": self.question_generator.retriever is not None
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
enhanced_rag_service = EnhancedRAGService() 